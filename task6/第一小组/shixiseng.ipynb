{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import random as rd\n",
    "import re\n",
    "from fake_useragent import UserAgent\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from lxml import etree\n",
    "ua = UserAgent()\n",
    "url_root = 'https://xiaoyuan.shixiseng.com/wangshen'\n",
    "header = {\n",
    "        'Accept': 'text/html,*/*',\n",
    "        'Accept-Encoding': 'gzip, deflate, br',\n",
    "        'Accept-Language': 'zh-CN,zh;q=0.9',\n",
    "        'Connection': 'keep-alive',\n",
    "        'User-Agent': ua.random,\n",
    "        'X-Requested-With': 'XMLHttpRequest'\n",
    "    }\n",
    "url_list = [0]*600\n",
    "for i in range(1,599):\n",
    "    url_list[i] = url_root+\"?p=%s\" %i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 初始化\n",
    "url_list = [0]*600\n",
    "errors = [0]*600\n",
    "error_web = []\n",
    "comp_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success 10\n",
      "\n",
      "success 20\n",
      "\n",
      "success 30\n",
      "\n",
      "success 40\n",
      "\n",
      "success 50\n",
      "\n",
      "success 60\n",
      "\n",
      "success 70\n",
      "\n",
      "success 80\n",
      "\n",
      "success 90\n",
      "\n",
      "success 100\n",
      "\n",
      "success 110\n",
      "\n",
      "success 120\n",
      "\n",
      "success 130\n",
      "\n",
      "success 140\n",
      "\n",
      "success 150\n",
      "\n",
      "success 160\n",
      "\n",
      "success 170\n",
      "\n",
      "success 180\n",
      "\n",
      "success 190\n",
      "\n",
      "success 200\n",
      "\n",
      "success 210\n",
      "\n",
      "success 220\n",
      "\n",
      "success 230\n",
      "\n",
      "success 240\n",
      "\n",
      "success 250\n",
      "\n",
      "success 260\n",
      "\n",
      "success 270\n",
      "\n",
      "success 280\n",
      "\n",
      "success 290\n",
      "\n",
      "success 300\n",
      "\n",
      "success 310\n",
      "\n",
      "success 320\n",
      "\n",
      "success 330\n",
      "\n",
      "success 340\n",
      "\n",
      "success 350\n",
      "\n",
      "success 360\n",
      "\n",
      "success 370\n",
      "\n",
      "success 380\n",
      "\n",
      "success 390\n",
      "\n",
      "success 400\n",
      "\n",
      "success 410\n",
      "\n",
      "success 420\n",
      "\n",
      "success 430\n",
      "\n",
      "success 440\n",
      "\n",
      "success 450\n",
      "\n",
      "success 460\n",
      "\n",
      "success 470\n",
      "\n",
      "success 480\n",
      "\n",
      "success 490\n",
      "\n",
      "success 500\n",
      "\n",
      "success 510\n",
      "\n",
      "success 520\n",
      "\n",
      "success 530\n",
      "\n",
      "success 540\n",
      "\n",
      "success 550\n",
      "\n",
      "success 560\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## 爬取公司主页网址\n",
    "for i in range(1,563):\n",
    "    r = requests.get(url = url_list[i], headers = header)\n",
    "    if (r.status_code == 200):\n",
    "        comp_page = r.text\n",
    "        webs = etree.HTML(comp_page)\n",
    "        urls = webs.xpath('//div[3]/div[1]/table/tr/td[2]/p[1]/a/@href')\n",
    "        for j in urls:\n",
    "            comp_list.append('https://xiaoyuan.shixiseng.com'+j)\n",
    "        if (i % 10 == 0):\n",
    "            print(\"success %s\\n\" %i)\n",
    "    else:\n",
    "        errors[i] = 1\n",
    "        error_web.append(url_list[i])\n",
    "        print(\"error: %s\\n\" %i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5810\n"
     ]
    }
   ],
   "source": [
    "# 获取的公司数量\n",
    "print(len(comp_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "##保存公司主页网址\n",
    "file = open(\"comp_list.txt\",\"w\")\n",
    "for i in comp_list:\n",
    "    file.write(i)\n",
    "    file.write(\"\\n\")\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "## try selenium（已废除）\n",
    "url_try = 'https://xiaoyuan.shixiseng.com/wangshen'\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import requests\n",
    "import random as rd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from pandas import DataFrame\n",
    "\n",
    "datamode = {\"company\":[],\"desc_com\":[],\"desc_coms\":[],\"career\":[],\"desc_car\":[],\"desc_cars\":[]}\n",
    "data_m = DataFrame(datamode)\n",
    "text = []\n",
    "errors = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time ##selenium（已废除）\n",
    "from lxml import etree\n",
    "driver = webdriver.Chrome(r'C:\\Program Files (x86)\\Google\\Chrome\\chromedriver_win32\\chromedriver')\n",
    "driver.get(url_try)\n",
    "\n",
    "url_list = []\n",
    "status = []\n",
    "loc = [0,0]\n",
    "num = 0\n",
    "webs = etree.HTML(driver.page_source)\n",
    "urls = webs.xpath('//div[3]/div[1]/table/tbody/tr/td[2]/p[1]/a/@href')\n",
    "\n",
    "for i in urls:\n",
    "    url_list.append('https://xiaoyuan.shixiseng.com'+i)\n",
    "\n",
    "for i in url_list:\n",
    "    driver.get(i)\n",
    "    add_tags = '//div[2]/div[1]/div[1]/ul/div/li[2]'\n",
    "    try:\n",
    "        loc[0] = 1\n",
    "        loc[1] = driver.find_element_by_xpath(add_tags) ##选取xpath元素\n",
    "    except Exception:\n",
    "        loc[0] = 0\n",
    "        loc[1] = driver.page_source\n",
    "        soups = BeautifulSoup(loc[1],\"html.parser\")\n",
    "        errors.append(soups)\n",
    "    else:\n",
    "        action = ActionChains(driver)\n",
    "        action.move_to_element(loc[1]).click().perform() ##移动鼠标并点击\n",
    "    \n",
    "        soups = BeautifulSoup(driver.page_source,\"html.parser\") ##分析html\n",
    "        res = soups.find_all(name = 'div', attrs = {'class':'detial-b no-border'})\n",
    "        for i in res:\n",
    "            text.append(i.get_text())\n",
    "        status.append(loc[0])\n",
    "        num = num + 1\n",
    "\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化公司主页文档记录\n",
    "detail_page = [\"\"]*6000 #原始网页信息\n",
    "comp_detail = [\"\"]*6000 #公司介绍\n",
    "carr_detail = [\"\"]*6000 #招聘信息\n",
    "comp_contents = [\"\"]*6000 #公司名称与所在地\n",
    "comp_website = [\"\"]*6000 #公司网址\n",
    "num = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading: 10\n",
      "loading: 20\n",
      "loading: 30\n",
      "loading: 40\n",
      "loading: 50\n",
      "loading: 60\n",
      "loading: 70\n",
      "loading: 80\n",
      "loading: 90\n",
      "loading: 100\n",
      "loading: 110\n",
      "loading: 120\n",
      "loading: 130\n",
      "loading: 140\n",
      "loading: 150\n",
      "loading: 160\n",
      "loading: 170\n",
      "loading: 180\n",
      "loading: 190\n",
      "loading: 200\n",
      "loading: 210\n",
      "loading: 220\n",
      "loading: 230\n",
      "loading: 240\n",
      "loading: 250\n",
      "loading: 260\n",
      "loading: 270\n",
      "loading: 280\n",
      "loading: 290\n",
      "loading: 300\n",
      "loading: 310\n",
      "loading: 320\n",
      "loading: 330\n",
      "loading: 340\n",
      "loading: 350\n",
      "loading: 360\n",
      "loading: 370\n",
      "loading: 380\n",
      "loading: 390\n",
      "loading: 400\n",
      "loading: 410\n",
      "loading: 420\n",
      "loading: 430\n",
      "loading: 440\n",
      "loading: 450\n",
      "loading: 460\n",
      "loading: 470\n",
      "loading: 480\n",
      "loading: 490\n",
      "loading: 500\n",
      "loading: 510\n",
      "loading: 520\n",
      "loading: 530\n",
      "loading: 540\n",
      "loading: 550\n",
      "loading: 560\n",
      "loading: 570\n",
      "loading: 580\n",
      "loading: 590\n",
      "loading: 600\n",
      "loading: 610\n",
      "loading: 620\n",
      "loading: 630\n",
      "loading: 640\n",
      "loading: 650\n",
      "loading: 660\n",
      "loading: 670\n",
      "loading: 680\n",
      "loading: 690\n",
      "loading: 700\n",
      "loading: 710\n",
      "loading: 720\n",
      "loading: 730\n",
      "loading: 740\n",
      "loading: 750\n",
      "loading: 760\n",
      "loading: 770\n",
      "loading: 780\n",
      "loading: 790\n",
      "loading: 800\n",
      "loading: 810\n",
      "loading: 820\n",
      "loading: 830\n",
      "loading: 840\n",
      "loading: 850\n",
      "loading: 860\n",
      "loading: 870\n",
      "loading: 880\n",
      "loading: 890\n",
      "loading: 900\n",
      "loading: 910\n",
      "loading: 920\n",
      "loading: 930\n",
      "loading: 940\n",
      "loading: 950\n",
      "loading: 960\n",
      "loading: 970\n",
      "loading: 980\n",
      "loading: 990\n",
      "loading: 1000\n",
      "loading: 1010\n",
      "loading: 1020\n",
      "loading: 1030\n",
      "loading: 1040\n",
      "loading: 1050\n",
      "loading: 1060\n",
      "loading: 1070\n",
      "loading: 1080\n",
      "loading: 1090\n",
      "loading: 1100\n",
      "loading: 1110\n",
      "loading: 1120\n",
      "loading: 1130\n",
      "loading: 1140\n",
      "loading: 1150\n",
      "loading: 1160\n",
      "loading: 1170\n",
      "loading: 1180\n",
      "loading: 1190\n",
      "loading: 1200\n",
      "loading: 1210\n",
      "loading: 1220\n",
      "loading: 1230\n",
      "loading: 1240\n",
      "loading: 1250\n",
      "loading: 1260\n",
      "loading: 1270\n",
      "loading: 1280\n",
      "loading: 1290\n",
      "loading: 1300\n",
      "loading: 1310\n",
      "loading: 1320\n",
      "loading: 1330\n",
      "loading: 1340\n",
      "loading: 1350\n",
      "loading: 1360\n",
      "loading: 1370\n",
      "loading: 1380\n",
      "loading: 1390\n",
      "loading: 1400\n",
      "loading: 1410\n",
      "loading: 1420\n",
      "loading: 1430\n",
      "loading: 1440\n",
      "loading: 1450\n",
      "loading: 1460\n",
      "loading: 1470\n",
      "loading: 1480\n",
      "loading: 1490\n",
      "loading: 1500\n",
      "loading: 1510\n",
      "loading: 1520\n",
      "loading: 1530\n",
      "loading: 1540\n",
      "loading: 1550\n",
      "loading: 1560\n",
      "loading: 1570\n",
      "loading: 1580\n",
      "loading: 1590\n",
      "loading: 1600\n",
      "loading: 1610\n",
      "loading: 1620\n",
      "loading: 1630\n",
      "loading: 1640\n",
      "loading: 1650\n",
      "loading: 1660\n",
      "loading: 1670\n",
      "loading: 1680\n",
      "loading: 1690\n",
      "loading: 1700\n",
      "loading: 1710\n",
      "loading: 1720\n",
      "loading: 1730\n",
      "loading: 1740\n",
      "loading: 1750\n",
      "loading: 1760\n",
      "loading: 1770\n",
      "loading: 1780\n",
      "loading: 1790\n",
      "loading: 1800\n",
      "loading: 1810\n",
      "loading: 1820\n",
      "loading: 1830\n",
      "loading: 1840\n",
      "loading: 1850\n",
      "loading: 1860\n",
      "loading: 1870\n",
      "loading: 1880\n",
      "loading: 1890\n",
      "loading: 1900\n",
      "loading: 1910\n",
      "loading: 1920\n",
      "loading: 1930\n",
      "loading: 1940\n",
      "loading: 1950\n",
      "loading: 1960\n",
      "loading: 1970\n",
      "loading: 1980\n",
      "loading: 1990\n",
      "loading: 2000\n",
      "loading: 2010\n",
      "loading: 2020\n",
      "loading: 2030\n",
      "loading: 2040\n",
      "loading: 2050\n",
      "loading: 2060\n",
      "loading: 2070\n",
      "loading: 2080\n",
      "loading: 2090\n",
      "loading: 2100\n",
      "loading: 2110\n",
      "loading: 2120\n",
      "loading: 2130\n",
      "loading: 2140\n",
      "loading: 2150\n",
      "loading: 2160\n",
      "loading: 2170\n",
      "loading: 2180\n",
      "loading: 2190\n",
      "loading: 2200\n",
      "loading: 2210\n",
      "loading: 2220\n",
      "loading: 2230\n",
      "loading: 2240\n",
      "loading: 2250\n",
      "loading: 2260\n",
      "loading: 2270\n",
      "loading: 2280\n",
      "loading: 2290\n",
      "loading: 2300\n",
      "loading: 2310\n",
      "loading: 2320\n",
      "loading: 2330\n",
      "loading: 2340\n",
      "loading: 2350\n",
      "loading: 2360\n",
      "loading: 2370\n",
      "loading: 2380\n",
      "loading: 2390\n",
      "loading: 2400\n",
      "loading: 2410\n",
      "loading: 2420\n",
      "loading: 2430\n",
      "loading: 2440\n",
      "loading: 2450\n",
      "loading: 2460\n",
      "loading: 2470\n",
      "loading: 2480\n",
      "loading: 2490\n",
      "loading: 2500\n",
      "loading: 2510\n",
      "loading: 2520\n",
      "loading: 2530\n",
      "loading: 2540\n",
      "loading: 2550\n",
      "loading: 2560\n",
      "loading: 2570\n",
      "loading: 2580\n",
      "loading: 2590\n",
      "loading: 2600\n",
      "loading: 2610\n",
      "loading: 2620\n",
      "loading: 2630\n",
      "loading: 2640\n",
      "loading: 2650\n",
      "loading: 2660\n",
      "loading: 2670\n",
      "loading: 2680\n",
      "loading: 2690\n",
      "loading: 2700\n",
      "loading: 2710\n",
      "loading: 2720\n",
      "loading: 2730\n",
      "loading: 2740\n",
      "loading: 2750\n",
      "loading: 2760\n",
      "loading: 2770\n",
      "loading: 2780\n",
      "loading: 2790\n",
      "loading: 2800\n",
      "loading: 2810\n",
      "loading: 2820\n",
      "loading: 2830\n",
      "loading: 2840\n",
      "loading: 2850\n",
      "loading: 2860\n",
      "loading: 2870\n",
      "loading: 2880\n",
      "loading: 2890\n",
      "loading: 2900\n",
      "loading: 2910\n",
      "loading: 2920\n",
      "loading: 2930\n",
      "loading: 2940\n",
      "loading: 2950\n",
      "loading: 2960\n",
      "loading: 2970\n",
      "loading: 2980\n",
      "loading: 2990\n",
      "loading: 3000\n",
      "loading: 3010\n",
      "loading: 3020\n",
      "loading: 3030\n",
      "loading: 3040\n",
      "loading: 3050\n",
      "loading: 3060\n",
      "loading: 3070\n",
      "loading: 3080\n",
      "loading: 3090\n",
      "loading: 3100\n",
      "loading: 3110\n",
      "loading: 3120\n",
      "loading: 3130\n",
      "loading: 3140\n",
      "loading: 3150\n",
      "loading: 3160\n",
      "loading: 3170\n",
      "loading: 3180\n",
      "loading: 3190\n",
      "loading: 3200\n",
      "loading: 3210\n",
      "loading: 3220\n",
      "loading: 3230\n",
      "loading: 3240\n",
      "loading: 3250\n",
      "loading: 3260\n",
      "loading: 3270\n",
      "loading: 3280\n",
      "loading: 3290\n",
      "loading: 3300\n",
      "loading: 3310\n",
      "loading: 3320\n",
      "loading: 3330\n",
      "loading: 3340\n",
      "loading: 3350\n",
      "loading: 3360\n",
      "loading: 3370\n",
      "loading: 3380\n",
      "loading: 3390\n",
      "loading: 3400\n",
      "loading: 3410\n",
      "loading: 3420\n",
      "loading: 3430\n",
      "loading: 3440\n",
      "loading: 3450\n",
      "loading: 3460\n",
      "loading: 3470\n",
      "loading: 3480\n",
      "loading: 3490\n",
      "loading: 3500\n",
      "loading: 3510\n",
      "loading: 3520\n",
      "loading: 3530\n",
      "loading: 3540\n",
      "loading: 3550\n",
      "loading: 3560\n",
      "loading: 3570\n",
      "loading: 3580\n",
      "loading: 3590\n",
      "loading: 3600\n",
      "loading: 3610\n",
      "loading: 3620\n",
      "loading: 3630\n",
      "loading: 3640\n",
      "loading: 3650\n",
      "loading: 3660\n",
      "loading: 3670\n",
      "loading: 3680\n",
      "loading: 3690\n",
      "loading: 3700\n",
      "loading: 3710\n",
      "loading: 3720\n",
      "loading: 3730\n",
      "loading: 3740\n",
      "loading: 3750\n",
      "loading: 3760\n",
      "loading: 3770\n",
      "loading: 3780\n",
      "loading: 3790\n",
      "loading: 3800\n",
      "loading: 3810\n",
      "loading: 3820\n",
      "loading: 3830\n",
      "loading: 3840\n",
      "loading: 3850\n",
      "loading: 3860\n",
      "loading: 3870\n",
      "loading: 3880\n",
      "loading: 3890\n",
      "loading: 3900\n",
      "loading: 3910\n",
      "loading: 3920\n",
      "loading: 3930\n",
      "loading: 3940\n",
      "loading: 3950\n",
      "loading: 3960\n",
      "loading: 3970\n",
      "loading: 3980\n",
      "loading: 3990\n",
      "loading: 4000\n",
      "loading: 4010\n",
      "loading: 4020\n",
      "loading: 4030\n",
      "loading: 4040\n",
      "loading: 4050\n",
      "loading: 4060\n",
      "loading: 4070\n",
      "loading: 4080\n",
      "loading: 4090\n",
      "loading: 4100\n",
      "loading: 4110\n",
      "loading: 4120\n",
      "loading: 4130\n",
      "loading: 4140\n",
      "loading: 4150\n",
      "loading: 4160\n",
      "loading: 4170\n",
      "loading: 4180\n",
      "loading: 4190\n",
      "loading: 4200\n",
      "loading: 4210\n",
      "loading: 4220\n",
      "loading: 4230\n",
      "loading: 4240\n",
      "loading: 4250\n",
      "loading: 4260\n",
      "loading: 4270\n",
      "loading: 4280\n",
      "loading: 4290\n",
      "loading: 4300\n",
      "loading: 4310\n",
      "loading: 4320\n",
      "loading: 4330\n",
      "loading: 4340\n",
      "loading: 4350\n",
      "loading: 4360\n",
      "loading: 4370\n",
      "loading: 4380\n",
      "loading: 4390\n",
      "loading: 4400\n",
      "loading: 4410\n",
      "loading: 4420\n"
     ]
    }
   ],
   "source": [
    "# 重爬取\n",
    "import time\n",
    "addrs = \"C:\\\\Users\\\\86157\\\\Desktop\"\n",
    "detail_page = [\"\"]*6000 #原始网页信息\n",
    "num = 1\n",
    "file = open(\"comp_list.txt\",\"r\")\n",
    "lines = file.readlines()\n",
    "file.close()\n",
    "for i in lines:\n",
    "    i = i.strip('\\n')\n",
    "    r = requests.get(url = i, headers = header)\n",
    "    if (r.status_code == 200):\n",
    "        detail_page[num] = r.text\n",
    "        locate = addrs +'\\\\raw_data\\\\'+ str(num) + '.txt'\n",
    "        file_w = open(locate, 'wb+')\n",
    "        for j in detail_page[num]:\n",
    "            file_w.write(str(j).encode('utf-8'))\n",
    "            file_w.write(\"\\n\".encode('utf-8'))\n",
    "        file_w.close()\n",
    "        num = num + 1\n",
    "        if (num % 10 == 0):\n",
    "            print(\"loading: %s\" %num)\n",
    "        time.sleep(1)\n",
    "    else:\n",
    "        error_web.append(i)\n",
    "        print(\"error: %s\\n\" %i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading 100\n",
      "\n",
      "loading 200\n",
      "\n",
      "loading 300\n",
      "\n",
      "loading 400\n",
      "\n",
      "loading 500\n",
      "\n",
      "loading 600\n",
      "\n",
      "loading 700\n",
      "\n",
      "loading 800\n",
      "\n",
      "loading 900\n",
      "\n",
      "loading 1000\n",
      "\n",
      "loading 1100\n",
      "\n",
      "loading 1200\n",
      "\n",
      "loading 1300\n",
      "\n",
      "loading 1400\n",
      "\n",
      "loading 1500\n",
      "\n",
      "loading 1600\n",
      "\n",
      "loading 1700\n",
      "\n",
      "loading 1800\n",
      "\n",
      "loading 1900\n",
      "\n",
      "loading 2000\n",
      "\n",
      "loading 2100\n",
      "\n",
      "loading 2200\n",
      "\n",
      "loading 2300\n",
      "\n",
      "loading 2400\n",
      "\n",
      "loading 2500\n",
      "\n",
      "loading 2600\n",
      "\n",
      "loading 2700\n",
      "\n",
      "loading 2800\n",
      "\n",
      "loading 2900\n",
      "\n",
      "loading 3000\n",
      "\n",
      "loading 3100\n",
      "\n",
      "loading 3200\n",
      "\n",
      "loading 3300\n",
      "\n",
      "loading 3400\n",
      "\n",
      "loading 3500\n",
      "\n",
      "loading 3600\n",
      "\n",
      "loading 3700\n",
      "\n",
      "loading 3800\n",
      "\n",
      "loading 3900\n",
      "\n",
      "loading 4000\n",
      "\n",
      "loading 4100\n",
      "\n",
      "loading 4200\n",
      "\n",
      "loading 4300\n",
      "\n",
      "loading 4400\n",
      "\n",
      "loading 4500\n",
      "\n",
      "loading 4600\n",
      "\n",
      "loading 4700\n",
      "\n",
      "loading 4800\n",
      "\n",
      "loading 4900\n",
      "\n",
      "loading 5000\n",
      "\n",
      "loading 5100\n",
      "\n",
      "loading 5200\n",
      "\n",
      "loading 5300\n",
      "\n",
      "loading 5400\n",
      "\n",
      "loading 5500\n",
      "\n",
      "loading 5600\n",
      "\n",
      "loading 5700\n",
      "\n",
      "loading 5800\n",
      "\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\86157\\\\Desktop\\\\raw_data\\\\5810.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-89-db9c6f03cb24>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnums\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mfile_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maddrs\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'.txt'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0mpage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mnew_p\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\86157\\\\Desktop\\\\raw_data\\\\5810.txt'"
     ]
    }
   ],
   "source": [
    "# 重清洗\n",
    "addrs = \"C:\\\\Users\\\\86157\\\\Desktop\\\\raw_data\\\\\"\n",
    "comp_addr = \"C:\\\\Users\\\\86157\\\\Desktop\\\\comp_data\\\\\"\n",
    "written = comp_addr + \"contents.txt\"\n",
    "dats = open(written, \"wb+\")\n",
    "nums = 5810\n",
    "for i in range(1,nums+1):\n",
    "    file_name = addrs+ str(i) + '.txt'\n",
    "    file = open(file_name, \"r\",encoding = 'utf-8')\n",
    "    page = file.readlines()\n",
    "    new_p = \"\"\n",
    "    for line in page:\n",
    "        new_p = new_p +line.strip(\"\\n\")\n",
    "    webs = etree.HTML(new_p)\n",
    "    comp_detail = webs.xpath(\"//div[2]/div[1]/div[2]/div/div[2]/div/div[@class ='detial-l']/div[2]/p/text()\")\n",
    "    carr_detail_raw = webs.xpath('//div[2]/div[1]/div[2]/div/div[2]/div/div[1]/div[3]/div/p')\n",
    "    carr_detail = []\n",
    "    for j in carr_detail_raw:\n",
    "        carr_detail.append(etree.tostring(j, encoding = 'Unicode'))\n",
    "    comp_contents = webs.xpath('//div[2]/div[2]/p/text()')\n",
    "    comp_website = webs.xpath('//div[2]/div[2]/a/@data-href')\n",
    "    \n",
    "    for j in comp_contents:\n",
    "        dats.write(str(j).encode('utf-8'))\n",
    "        dats.write('\\r\\n'.encode('utf-8'))\n",
    "    for j in comp_website:\n",
    "        dats.write(str(j).encode('utf-8'))\n",
    "        dats.write('\\r\\n'.encode('utf-8'))\n",
    "    for j in comp_detail:\n",
    "        dats.write(str(j).encode('utf-8'))\n",
    "        dats.write('\\r\\n'.encode('utf-8'))\n",
    "    for j in carr_detail:\n",
    "        dats.write(str(j).encode('utf-8'))\n",
    "        dats.write('\\r\\n'.encode('utf-8'))\n",
    "    dats.write(\"-----------------------------------------------------------------------------\\n\".encode('utf-8'))\n",
    "    file.close()\n",
    "    if (i % 100 == 0):\n",
    "        print(\"loading %s\\n\" %i)\n",
    "dats.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mode carrdetail部分清洗处理 - 初始化\n",
    "import pandas as pd\n",
    "compdata = {\"company\":[],\"comp_locate\":[],\"comp_detail\":[],\"comp_website\":[],\"carr_name\":[],\"carr_doi\":[],\"carr_ask\":[],\"carr_desc\":[]}\n",
    "compdata = pd.DataFrame(compdata)\n",
    "file_name = \"C:\\\\Users\\\\86157\\\\Desktop\\\\raw_data\\\\1015.txt\"\n",
    "file = open(file_name, \"r\",encoding = 'utf-8')\n",
    "page = file.readlines()\n",
    "new_p = \"\"\n",
    "for line in page:\n",
    "    new_p = new_p +line.strip(\"\\n\")\n",
    "webs = etree.HTML(new_p)\n",
    "comp_detail = webs.xpath(\"//div[2]/div[1]/div[2]/div/div[2]/div/div[@class ='detial-l']/div[2]/p/text()\")\n",
    "carr_detail_raw = webs.xpath('//div[2]/div[1]/div[2]/div/div[2]/div/div[1]/div[3]/div/p')\n",
    "carr_detail = []\n",
    "for j in carr_detail_raw:\n",
    "    carr_detail.append(etree.tostring(j, encoding = 'Unicode'))\n",
    "comp_contents = webs.xpath('//div[2]/div[2]/p/text()')\n",
    "comp_website = webs.xpath('//div[2]/div[2]/a/@data-href')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n",
      "['职责：']\n",
      "<p><span style=\"font-size:14px\">1、为客户提供技术支持及服务，包括设备维护，数据分析，故障处理等；</span></p>\n",
      "<p><span style=\"font-size:14px\">2、公司内部技术支持；</span></p>\n",
      "<p><span style=\"font-size:14px\">3、为内外部客户提供技术培训；</span></p>\n",
      "<p><span style=\"font-size:14px\">4、承担公司安排的其它工作；</span></p>\n",
      "8\n",
      "['要求：']\n",
      "<p><span style=\"font-size:14px\">1、本科学历，自动化/机电一体化/电子工程/电气工程及其自动化/机械制造及其自动化/精密仪器/光学等相关专业；</span></p>\n",
      "<p><span style=\"font-size:14px\">2、有效的沟通和人际交往能力；</span></p>\n",
      "<p><span style=\"font-size:14px\">3、较强的责任心与团队合作精神，快速学习的能力以及承受富有挑战性工作的能力；</span></p>\n",
      "<p><span style=\"font-size:14px\">4、良好的英语听说读写能力。</span></p>\n",
      "<p> </p>\n",
      "14\n",
      "['职责：']\n",
      "<p><span style=\"font-size:14px\">1、在客户端支持工厂自动化相关软件开发，疑难解答与性能维护等服务；</span></p>\n",
      "<p><span style=\"font-size:14px\">2、整合机台软件链接客户计算机集成制造 (CIM) 系统，进行全面自动化生产；</span></p>\n",
      "<p><span style=\"font-size:14px\">3、参与新机台平台软件的支持或开发；</span></p>\n",
      "<p><span style=\"font-size:14px\">4、开发定制化项目与产品来满足客户的需求；</span></p>\n",
      "21\n",
      "['要求：']\n",
      "<p><span style=\"font-size:14px\">1、硕士学历，信息科学与工程/计算机工程与科学/自动化/机电一体化/电子工程/电气工程及其自动化等相关专业；</span></p>\n",
      "<p><span style=\"font-size:14px\">2、有效的沟通和人际交往能力；</span></p>\n",
      "<p><span style=\"font-size:14px\">3、较强的责任心与团队合作精神，快速学习的能力以及承受富有挑战性工作的能力；</span></p>\n",
      "<p><span style=\"font-size:14px\">4、良好的英语听说读写能力。</span></p>\n",
      "26\n"
     ]
    }
   ],
   "source": [
    "# mode carrdetail部分清洗处理 - 主体\n",
    "import re\n",
    "carr_name = []\n",
    "carr_doi = []\n",
    "carr_ask = []\n",
    "carr_desc = []\n",
    "carr_indx = []\n",
    "length = len(carr_detail)\n",
    "print(length)\n",
    "tag = 0\n",
    "carr_name_re = re.compile(u'<strong>([\\u4e00-\\u9fa5]+a-zA-Z)</strong>')\n",
    "carr_doi_re = re.compile(u'[<strong>]?(职责[:： ]*)[</strong>]?')\n",
    "carr_ask_re = re.compile(u'[<strong>]?(要求[:： ]*)[</strong>]?')\n",
    "carr_desc_re = re.compile(u'[<strong>]?(说明[:： ]*)[</strong>]?')\n",
    "carr_stop_re = re.compile(u'<strong>|职责|要求|说明')\n",
    "\n",
    "for i in range(0,length):\n",
    "    if (len(carr_name_re.findall(carr_detail[i])) >0):\n",
    "        print(carr_name_re.findall(carr_detail[i]))\n",
    "        \n",
    "    if (len(carr_doi_re.findall(carr_detail[i])) >0):\n",
    "        print(carr_doi_re.findall(carr_detail[i]))\n",
    "        tag = i + 1\n",
    "        if (tag >= length):\n",
    "            break\n",
    "        while ((len(carr_stop_re.findall(carr_detail[tag])) == 0) and (tag<length)):\n",
    "            print(carr_detail[tag])\n",
    "            tag = tag + 1\n",
    "            if (tag >= length):\n",
    "                break\n",
    "        print(tag)\n",
    "\n",
    "    if (len(carr_ask_re.findall(carr_detail[i])) >0):\n",
    "        print(carr_ask_re.findall(carr_detail[i]))\n",
    "        tag = i + 1\n",
    "        if (tag >= length):\n",
    "            break\n",
    "        while ((len(carr_stop_re.findall(carr_detail[tag])) == 0) and (tag<length)):\n",
    "            print(carr_detail[tag])\n",
    "            tag = tag + 1\n",
    "            if (tag >= length):\n",
    "                break\n",
    "        print(tag)\n",
    "\n",
    "    if (len(carr_desc_re.findall(carr_detail[i])) >0):\n",
    "        print(carr_desc_re.findall(carr_detail[i]))\n",
    "        tag = i + 1\n",
    "        if (tag >= length):\n",
    "            break\n",
    "        while ((len(carr_stop_re.findall(carr_detail[tag])) == 0) and (tag<length)):\n",
    "            print(carr_detail[tag])\n",
    "            tag = tag + 1\n",
    "            if (tag >= length):\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<p><strong>在招职位：</strong>\\xa0</p>', '<p><strong><span style=\"font-size:14px\">Customer Engineer\\xa0现场客户服务工程师</span></strong></p>', '<p><span style=\"font-size:14px\">（工作地点：上海、南京、无锡、合肥、广州、深圳、西安、武汉）\\xa0</span></p>', '<p><span style=\"font-size:14px\">主要职责：</span></p>', '<p><span style=\"font-size:14px\">1、为客户提供技术支持及服务，包括设备维护，数据分析，故障处理等；</span></p>', '<p><span style=\"font-size:14px\">2、公司内部技术支持；</span></p>', '<p><span style=\"font-size:14px\">3、为内外部客户提供技术培训；</span></p>', '<p><span style=\"font-size:14px\">4、承担公司安排的其它工作；</span></p>', '<p><span style=\"font-size:14px\">岗位要求：</span></p>', '<p><span style=\"font-size:14px\">1、本科学历，自动化/机电一体化/电子工程/电气工程及其自动化/机械制造及其自动化/精密仪器/光学等相关专业；</span></p>', '<p><span style=\"font-size:14px\">2、有效的沟通和人际交往能力；</span></p>', '<p><span style=\"font-size:14px\">3、较强的责任心与团队合作精神，快速学习的能力以及承受富有挑战性工作的能力；</span></p>', '<p><span style=\"font-size:14px\">4、良好的英语听说读写能力。</span></p>', '<p>\\xa0</p>', '<p><strong><span style=\"font-size:14px\">Software Engineer\\xa0软件工程师</span></strong></p>', '<p><span style=\"font-size:14px\">（工作地点：西安、上海）</span></p>', '<p><span style=\"font-size:14px\">主要职责：</span></p>', '<p><span style=\"font-size:14px\">1、在客户端支持工厂自动化相关软件开发，疑难解答与性能维护等服务；</span></p>', '<p><span style=\"font-size:14px\">2、整合机台软件链接客户计算机集成制造 (CIM) 系统，进行全面自动化生产；</span></p>', '<p><span style=\"font-size:14px\">3、参与新机台平台软件的支持或开发；</span></p>', '<p><span style=\"font-size:14px\">4、开发定制化项目与产品来满足客户的需求；</span></p>', '<p><span style=\"font-size:14px\">岗位要求：</span></p>', '<p><span style=\"font-size:14px\">1、硕士学历，信息科学与工程/计算机工程与科学/自动化/机电一体化/电子工程/电气工程及其自动化等相关专业；</span></p>', '<p><span style=\"font-size:14px\">2、有效的沟通和人际交往能力；</span></p>', '<p><span style=\"font-size:14px\">3、较强的责任心与团队合作精神，快速学习的能力以及承受富有挑战性工作的能力；</span></p>', '<p><span style=\"font-size:14px\">4、良好的英语听说读写能力。</span></p>']\n"
     ]
    }
   ],
   "source": [
    "# 监测输出窗口\n",
    "print(carr_detail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['游戏研发工程师'], ['人工智能及大数据研发工程师'], ['游戏策划助理'], ['网申入口']]\n"
     ]
    }
   ],
   "source": [
    "carr_nums = []\n",
    "carr_indx = []\n",
    "carr_re = re.compile(u'<strong>([\\u4e00-\\u9fa5]+)</strong>')\n",
    "for i in range(0, len(carr_detail)):\n",
    "    if (len(carr_re.findall(carr_detail[i])) >0 ):\n",
    "        carr_nums.append(carr_re.findall(carr_detail[i]))\n",
    "        carr_indx.append(i)\n",
    "\n",
    "print(carr_nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['岗位职责：']\n",
      "<p>设计游戏架构与各大功能，制作游戏开发及运维工具，提供相关技术支持，研究尖端游戏引擎，拓展技术，提升游戏品质。 </p>\n",
      "\n",
      "\n",
      "['岗位职责：']\n",
      "<p>研发大数据分析处理平台，研究数据挖掘算法技术及应用、人工智能领域相关算法等，开发自然语言处理、图像识别等方向的相关应用，面向游戏及其他领域产品提供大数据解决方案。</p>\n",
      "\n",
      "\n",
      "['岗位职责：']\n",
      "<p>1、参与游戏规则、玩法的讨论、设计和跟进制作；<br/>\n",
      "2、游戏各模块相关文档的撰写与维护；<br/>\n",
      "3、对游戏的各种资源进行整理、配置和维护；<br/>\n",
      "4、设计、关注和调整游戏生态环境与经济体系的运作；<br/>\n",
      "5、对各数值模块建立数学模型，进行演算和公式设计，对游戏各数值模块进行维护调整；<br/>\n",
      "6、构建游戏世界观，进行剧情、对白、道具和技能描述等相关文案工作，完成文化包装。<br/>\n",
      "岗位发展方向：<br/>\n",
      "游戏策划：系统、数值、玩法、交互设计方向</p>\n",
      "\n",
      "\n",
      "['岗位要求：']\n",
      "<p>1、2020届毕业生，计算机、数学或相关专业；<br/>\n",
      "2、计算机基础扎实，熟练掌握至少一门编程语言：C/C++、Java、Python、C#、javascript、lua；<br/>\n",
      "3、有良好的逻辑思维和自学能力，良好的编程风格规范，基本的英文文档阅读能力；<br/>\n",
      "4、热爱游戏，善于分析和解决问题，勇于探索不同方法来解决难题；<br/>\n",
      "5、能承受工作压力，有团队合作精神。<br/>\n",
      "有以下经历或者经验者优先：<br/>\n",
      "1、游戏引擎使用经验：UE4、Unity、Cocos2d-x；<br/>\n",
      "2、熟悉图形学方面知识，具备各大图形库使用经验；<br/>\n",
      "3、熟悉网络协议，socket编程；<br/>\n",
      "4、ACM比赛或者其他软件类比大赛获奖经历；<br/>\n",
      "5、大型项目经验 。</p>\n",
      "\n",
      "\n",
      "['岗位要求：']\n",
      "<p>1、2020届毕业生，数学或计算机相关专业；<br/>\n",
      "2、计算机基础知识扎实，精通算法设计/数据结构，熟悉JAVA或C/C++语言编程；<br/>\n",
      "3、熟悉机器学习常用算法，熟悉Linux/Unix平台上的开发环境；<br/>\n",
      "4、在以下某一领域有一定经验：数据挖掘、自然语言处理、图像识别、语音识别；<br/>\n",
      "5、优秀的学习能力，善于探索钻研。</p>\n",
      "\n",
      "\n",
      "['岗位要求：']\n",
      "<p>2020届应届毕业生，要求符合以下条件2条或以上<br/>\n",
      "1、热爱游戏，游戏经历极为丰富；<br/>\n",
      "2、游戏历程在娱乐之余，还喜爱研究游戏细节；<br/>\n",
      "3、心态谦谨，勤奋好学；<br/>\n",
      "4、具备较好的逻辑能力，有独立的思考和看法；<br/>\n",
      "5、沟通能力好，书面与口头表达流畅严谨，易交流，心态平稳不浮躁。<br/>\n",
      "有以下条件尤佳：<br/>\n",
      "1、知识面广，对各学科知识、各类型文化有较多涉猎；<br/>\n",
      "2、阅读经验丰富，对网络文学和传统文学均有较好了解；<br/>\n",
      "3、有过项目开发、MOD制作、汉化制作等相关经验 。</p>\n",
      "\n",
      "\n",
      "['岗位说明：']\n",
      "<p>招聘人数： 广州76人   成都16人  </p>\n",
      "\n",
      "\n",
      "<p>薪资待遇：12K~18K</p>\n",
      "\n",
      "\n",
      "<p> </p>\n",
      "\n",
      "\n",
      "['岗位说明：']\n",
      "<p>招聘人数： 广州10人  </p>\n",
      "\n",
      "\n",
      "<p>薪资待遇：12K~18K</p>\n",
      "\n",
      "\n",
      "<p> </p>\n",
      "\n",
      "\n",
      "['岗位说明：']\n",
      "<p>招聘人数： 广州36人   成都6人  </p>\n",
      "\n",
      "\n",
      "<p>薪资待遇：6K~10K</p>\n",
      "\n",
      "\n",
      "<p> </p>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 分离\n",
    "\n",
    "loca_re = re.compile(u'<strong>(岗位职责[:： ]*)</strong>')\n",
    "stop_re = re.compile(u'<strong>')\n",
    "for i in range(0, len(carr_detail)):\n",
    "    if (len(loca_re.findall(carr_detail[i])) >0 ):\n",
    "        print(loca_re.findall(carr_detail[i]))\n",
    "        tag = i + 1\n",
    "        while (len(stop_re.findall(carr_detail[tag])) == 0):\n",
    "            print(carr_detail[tag])\n",
    "            tag = tag + 1\n",
    "\n",
    "ask_re = re.compile(u'<strong>(岗位要求[:： ]*)</strong>')\n",
    "stop_re = re.compile(u'<strong>')\n",
    "for i in range(0, len(carr_detail)):\n",
    "    if (len(ask_re.findall(carr_detail[i])) >0 ):\n",
    "        print(ask_re.findall(carr_detail[i]))\n",
    "        tag = i + 1\n",
    "        while (len(stop_re.findall(carr_detail[tag])) == 0):\n",
    "            print(carr_detail[tag])\n",
    "            tag = tag + 1\n",
    "            \n",
    "desc_re = re.compile(u'<strong>(岗位说明[:： ]*)</strong>')\n",
    "stop_re = re.compile(u'<strong>')\n",
    "for i in range(0, len(carr_detail)):\n",
    "    if (len(desc_re.findall(carr_detail[i])) >0 ):\n",
    "        print(desc_re.findall(carr_detail[i]))\n",
    "        tag = i + 1\n",
    "        while (len(stop_re.findall(carr_detail[tag])) == 0):\n",
    "            print(carr_detail[tag])\n",
    "            tag = tag + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<p><strong>在招职位：</strong>\\xa0</p>\\n\\n', '<p><img alt=\"多益网络2020校园招聘\" src=\"https://xyzimg.xiaoyuanzhao.com/3F/6B/3FDB8599A05C98A3C8A72CBA6623056B.png\" style=\"height:206px; width:750px\"/></p>\\n\\n', '<p><strong>游戏研发工程师</strong></p>\\n\\n', '<p><strong>岗位职责：</strong></p>\\n\\n', '<p>设计游戏架构与各大功能，制作游戏开发及运维工具，提供相关技术支持，研究尖端游戏引擎，拓展技术，提升游戏品质。\\xa0</p>\\n\\n', '<p><strong>岗位要求：</strong></p>\\n\\n', '<p>1、2020届毕业生，计算机、数学或相关专业；<br/>\\n2、计算机基础扎实，熟练掌握至少一门编程语言：C/C++、Java、Python、C#、javascript、lua；<br/>\\n3、有良好的逻辑思维和自学能力，良好的编程风格规范，基本的英文文档阅读能力；<br/>\\n4、热爱游戏，善于分析和解决问题，勇于探索不同方法来解决难题；<br/>\\n5、能承受工作压力，有团队合作精神。<br/>\\n有以下经历或者经验者优先：<br/>\\n1、游戏引擎使用经验：UE4、Unity、Cocos2d-x；<br/>\\n2、熟悉图形学方面知识，具备各大图形库使用经验；<br/>\\n3、熟悉网络协议，socket编程；<br/>\\n4、ACM比赛或者其他软件类比大赛获奖经历；<br/>\\n5、大型项目经验\\xa0。</p>\\n\\n', '<p><strong>岗位说明：</strong></p>\\n\\n', '<p>招聘人数：\\xa0广州76人\\xa0\\xa0\\xa0成都16人\\xa0\\xa0</p>\\n\\n', '<p>薪资待遇：12K~18K</p>\\n\\n', '<p>\\xa0</p>\\n\\n', '<p><strong>人工智能及大数据研发工程师</strong></p>\\n\\n', '<p><strong>岗位职责：</strong></p>\\n\\n', '<p>研发大数据分析处理平台，研究数据挖掘算法技术及应用、人工智能领域相关算法等，开发自然语言处理、图像识别等方向的相关应用，面向游戏及其他领域产品提供大数据解决方案。</p>\\n\\n', '<p><strong>岗位要求：</strong></p>\\n\\n', '<p>1、2020届毕业生，数学或计算机相关专业；<br/>\\n2、计算机基础知识扎实，精通算法设计/数据结构，熟悉JAVA或C/C++语言编程；<br/>\\n3、熟悉机器学习常用算法，熟悉Linux/Unix平台上的开发环境；<br/>\\n4、在以下某一领域有一定经验：数据挖掘、自然语言处理、图像识别、语音识别；<br/>\\n5、优秀的学习能力，善于探索钻研。</p>\\n\\n', '<p><strong>岗位说明：</strong></p>\\n\\n', '<p>招聘人数：\\xa0广州10人\\xa0\\xa0</p>\\n\\n', '<p>薪资待遇：12K~18K</p>\\n\\n', '<p>\\xa0</p>\\n\\n', '<p><strong>游戏策划助理</strong></p>\\n\\n', '<p><strong>岗位职责：</strong></p>\\n\\n', '<p>1、参与游戏规则、玩法的讨论、设计和跟进制作；<br/>\\n2、游戏各模块相关文档的撰写与维护；<br/>\\n3、对游戏的各种资源进行整理、配置和维护；<br/>\\n4、设计、关注和调整游戏生态环境与经济体系的运作；<br/>\\n5、对各数值模块建立数学模型，进行演算和公式设计，对游戏各数值模块进行维护调整；<br/>\\n6、构建游戏世界观，进行剧情、对白、道具和技能描述等相关文案工作，完成文化包装。<br/>\\n岗位发展方向：<br/>\\n游戏策划：系统、数值、玩法、交互设计方向</p>\\n\\n', '<p><strong>岗位要求：</strong></p>\\n\\n', '<p>2020届应届毕业生，要求符合以下条件2条或以上<br/>\\n1、热爱游戏，游戏经历极为丰富；<br/>\\n2、游戏历程在娱乐之余，还喜爱研究游戏细节；<br/>\\n3、心态谦谨，勤奋好学；<br/>\\n4、具备较好的逻辑能力，有独立的思考和看法；<br/>\\n5、沟通能力好，书面与口头表达流畅严谨，易交流，心态平稳不浮躁。<br/>\\n有以下条件尤佳：<br/>\\n1、知识面广，对各学科知识、各类型文化有较多涉猎；<br/>\\n2、阅读经验丰富，对网络文学和传统文学均有较好了解；<br/>\\n3、有过项目开发、MOD制作、汉化制作等相关经验 。</p>\\n\\n', '<p><strong>岗位说明：</strong></p>\\n\\n', '<p>招聘人数：\\xa0广州36人\\xa0\\xa0\\xa0成都6人\\xa0\\xa0</p>\\n\\n', '<p>薪资待遇：6K~10K</p>\\n\\n', '<p>\\xa0</p>\\n\\n', '<p>更多职位详情请点击“<span style=\"font-size:14px\"><strong>网申入口</strong></span>”查看\\u2003\\u2003</p>\\n', '<p><strong>招聘流程：</strong>\\xa0</p>\\n\\n', '<p><img alt=\"\" src=\"https://xyzimg.xiaoyuanzhao.com/F5/52/F5DC1DD03B8D3F9053AC668F7F18E752.png\" style=\"height:215px; width:750px\"/>\\u2003\\u2003</p>\\n', '<p><strong>Q&amp;A：</strong>\\xa0</p>\\n\\n', '<p><strong>一、网申</strong></p>\\n\\n', '<p>A:校招对象为毕业时间为2018年8月-2019年12月之间的同学（中国大陆院校以毕业证为准，港澳台及海外院校以学位证时间为准）。</p>\\n\\n', '<p>毕业时间为2018年8月前的同学，可参加多益网络社会招聘。多益网络社招网址。</p>\\n\\n', '<p>A:校招官网在线网申是多益网络接受求职申请的唯一途径，请务必通过校招官网提交简历。（宣讲会现场不接受纸质简历，支持移动端预投递）</p>\\n\\n', '<p>A：本次校招采取线下与线上相结合的形式，未能参加线下宣讲会的同学可以通过线上形式进入应聘流程。</p>\\n\\n', '<p>每位同学在同一招聘年度内，仅有两次网申机会，且无法连续两个季度投递简历，即可秋招、春招均投递简历，但不可冬招、春招投递简历。</p>\\n\\n', '<p>再次投递简历的同学可登录原帐号在校招官网选择岗位进行投递，当确认重新投递时，将以最新投递简历为准，进行新的应聘流程。</p>\\n\\n', '<p>A:笔试前可自行在个人中心修改简历内容（应聘岗位除外）。</p>\\n\\n', '<p>应聘岗位一经选择不可自行修改，需通过个人中心Q&amp;A中的留言板向HR提出申请，请仔细阅读岗位说明后选择合适岗位。</p>\\n\\n', '<p>A:只能申请一个职位。</p>\\n\\n', '<p><strong>二、笔试与面试</strong></p>\\n\\n', '<p>A:在笔试之前需完成简历投递、在线测评，通过后即可进行笔试。</p>\\n\\n', '<p>我们将根据笔试结果、简历情况、在线测评结果综合考虑应聘者是否能进入下一环节。</p>\\n\\n', '<p>A:线上笔试将在统一时间段进行，时间不可变动，但可在笔试开放时间段内选择合适时间开启考试。</p>\\n\\n', '<p>A:若通过笔试，将于笔试后的5-7个工作日内收到我们发出的面试通知；若通过面试，一般将于面试后的5-11个工作日内收到录用通知，届时请留意电话及短信通知。如人数较多，时间或有延后，请谅解。</p>\\n\\n', '<p>\\xa0</p>\\n\\n', '<p><strong>三、其他</strong></p>\\n\\n', '<p>A：一学年内最多有2次应聘机会，内推批次、技术提前批次不计入其中；连续的两个季度不可以同时投递（内推、技术提前批次除外）。</p>\\n\\n', '<p>A:多益网络校招官网、“多益网络招聘”微信公众提供最准确最完整的校园招聘信息。也可下载多益战盟APP，随时随地了解多益网络校招最新资讯。</p>\\n\\n', '<p>A:可直接登录电脑端校招官网的个人中心，或下载多益战盟APP，随时随地在移动端查看进度。</p>\\n\\n', '<p>A：可通过在线客服询问机器人，查看相应问题的回答即可。若仍旧未能解答你的疑问，请在登录帐号的状态下点击对话框右下角的留言板进行提问。hr看到后会尽快回答你的问题！\\u2003\\u2003</p>\\n']\n",
      "[['在招职位：'], ['游戏研发工程师'], ['岗位职责：'], ['岗位要求：'], ['岗位说明：'], ['人工智能及大数据研发工程师'], ['岗位职责：'], ['岗位要求：'], ['岗位说明：'], ['游戏策划助理'], ['岗位职责：'], ['岗位要求：'], ['岗位说明：'], ['网申入口'], ['招聘流程：'], ['Q&amp;A：'], ['一、网申'], ['二、笔试与面试'], ['三、其他']]\n"
     ]
    }
   ],
   "source": [
    "carr_detail_raw = webs.xpath('//div[2]/div[1]/div[2]/div/div[2]/div/div[1]/div[3]/div/p')\n",
    "carr_detail = []\n",
    "for j in carr_detail_raw:\n",
    "    carr_detail.append(etree.tostring(j, encoding = 'Unicode'))\n",
    "print(carr_detail)\n",
    "carr_doi = []\n",
    "carrdoi_re = re.compile(u'<strong>(.+)</strong>')\n",
    "for i in range(0, len(carr_detail)):\n",
    "    if (carrdoi_re.findall(carr_detail[i])):\n",
    "        carr_doi.append(carrdoi_re.findall(carr_detail[i]))\n",
    "print(carr_doi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"C:\\\\Users\\\\86157\\\\Desktop\\\\raw_data\\\\1.txt\"\n",
    "file = open(file_name, \"r\",encoding = 'utf-8')\n",
    "page = file.readlines()\n",
    "new_p = \"\"\n",
    "for line in page:\n",
    "    new_p = new_p +line.strip(\"\\n\")\n",
    "webs = etree.HTML(new_p)\n",
    "comp_detail = webs.xpath(\"//div[2]/div[1]/div[2]/div/div[2]/div/div[@class ='detial-l']/div[2]/p/text()\")\n",
    "carr_detail = webs.xpath('//div[2]/div[1]/div[2]/div/div[2]/div/div[1]/div[3]/div/p/text()')\n",
    "comp_contents = webs.xpath('//div[2]/div[2]/p/text()')\n",
    "comp_website = webs.xpath('//div[2]/div[2]/a/@data-href')\n",
    "\n",
    "comp_addr = \"C:\\\\Users\\\\86157\\\\Desktop\\\\comp_data\\\\\"\n",
    "written = comp_addr + \"contents.txt\"\n",
    "dats = open(written, \"wb+\")\n",
    "for i in comp_contents:\n",
    "    dats.write(str(i).encode('utf-8'))\n",
    "    dats.write('\\r\\n'.encode('utf-8'))\n",
    "for i in comp_website:\n",
    "    dats.write(str(i).encode('utf-8'))\n",
    "    dats.write('\\r\\n'.encode('utf-8'))\n",
    "for i in comp_detail:\n",
    "    dats.write(str(i).encode('utf-8'))\n",
    "    dats.write('\\r\\n'.encode('utf-8'))\n",
    "for i in carr_detail:\n",
    "    dats.write(str(i).encode('utf-8'))\n",
    "    dats.write('\\r\\n'.encode('utf-8'))\n",
    "dats.write(\"---------------------------------------------------------------------------------\\n\".encode('utf-8'))\n",
    "dats.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "occpdata = {\"company\":[],\"occupation\":[],\"location\":[],\"edu_ask\":[],\"scal\":[],\"detail\":[]}\n",
    "occpdata = pd.DataFrame(occpdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['公司信息', '湖北亿诺瑞\\uf885物制药有限公司', '黄冈']\n"
     ]
    }
   ],
   "source": [
    "print(comp_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存公司主页文件\n",
    "comp_detail_file = open(\"company_detail.txt\",\"wb+\")\n",
    "carr_detail_file = open(\"career_detail.txt\",\"wb+\")\n",
    "comp_contents_file = open(\"company_contents.txt\",\"wb+\")\n",
    "comp_website_file = open(\"company_website.txt\",\"wb+\")\n",
    "\n",
    "num_detail = 1\n",
    "for i in comp_detail:\n",
    "    comp_detail_file.write(str(num_detail).encode('utf-8'))\n",
    "    comp_detail_file.write('\\r\\n'.encode('utf-8'))\n",
    "    for j in i:\n",
    "        comp_detail_file.write(str(j).encode('utf-8'))\n",
    "        comp_detail_file.write('\\r\\n'.encode('utf-8'))\n",
    "    comp_detail_file.write('\\r\\n'.encode('utf-8'))\n",
    "    num_detail = num_detail + 1\n",
    "    \n",
    "num_detail = 1\n",
    "for i in carr_detail:\n",
    "    carr_detail_file.write(str(num_detail).encode('utf-8'))\n",
    "    carr_detail_file.write('\\r\\n'.encode('utf-8'))\n",
    "    for j in i:\n",
    "        carr_detail_file.write(str(j).encode('utf-8'))\n",
    "        carr_detail_file.write('\\r\\n'.encode('utf-8'))\n",
    "    carr_detail_file.write('\\r\\n'.encode('utf-8'))\n",
    "    num_detail = num_detail + 1\n",
    "\n",
    "num_detail = 1\n",
    "for i in comp_contents:\n",
    "    comp_contents_file.write(str(num_detail).encode('utf-8'))\n",
    "    comp_contents_file.write('\\r\\n'.encode('utf-8'))\n",
    "    for j in i:\n",
    "        comp_contents_file.write(str(j).encode('utf-8'))\n",
    "        comp_contents_file.write('\\r\\n'.encode('utf-8'))\n",
    "    comp_contents_file.write('\\r\\n'.encode('utf-8'))\n",
    "    num_detail = num_detail + 1\n",
    "    \n",
    "num_detail = 1\n",
    "for i in comp_website:\n",
    "    comp_detail_file.write(str(num_detail).encode('utf-8'))\n",
    "    comp_website_file.write(str(i).encode('utf-8'))\n",
    "    comp_website_file.write('\\r\\n'.encode('utf-8'))\n",
    "    num_detail = num_detail + 1\n",
    "\n",
    "comp_detail_file.close()\n",
    "carr_detail_file.close()\n",
    "comp_contents_file.close()\n",
    "comp_website_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重定向部分招聘信息\n",
    "new_addr = []\n",
    "occp_addr = [\"\"]*6000\n",
    "addrs = \"C:\\\\Users\\\\86157\\\\Desktop\\\\raw_data\\\\\"\n",
    "\n",
    "for i in range(1,nums):\n",
    "    file_name = addrs+ str(i) + '.txt'\n",
    "    file = open(file_name, \"r\",encoding = 'utf-8')\n",
    "    page = file.readlines()\n",
    "    new_p = \"\"\n",
    "    for line in page:\n",
    "        new_p = new_p +line.strip(\"\\n\")\n",
    "    webs = etree.HTML(new_p)\n",
    "    occp_addr[i] = webs.xpath('//div[2]/div[1]/div[3]/div[2]/div/div/a/@href')\n",
    "    \n",
    "for i in occp_addr:\n",
    "    if(len(i)>0):\n",
    "        for j in i:\n",
    "            new_addr.append(\"https://xiaoyuan.shixiseng.com\"+j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "484\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 爬取新信息\n",
    "import pandas as pd\n",
    "occpdata = {\"company\":[],\"occupation\":[],\"location\":[],\"edu_ask\":[],\"scal\":[],\"detail\":[]}\n",
    "occpdata = pd.DataFrame(occpdata)\n",
    "occp_page = [\"\"]*500\n",
    "occp_page_num = 0\n",
    "for i in new_addr:\n",
    "    r = requests.get(url = i, headers = header)\n",
    "    if (r.status_code == 200):\n",
    "        occp_page_num = occp_page_num + 1\n",
    "        occp_page[occp_page_num] = r.text\n",
    "        webs = etree.HTML(r.text)\n",
    "        occp_comp = webs.xpath(\"//div/div[2]/a/div/div/p[1]/text()\")\n",
    "        occp_name = webs.xpath(\"//div/div[1]/div[@class = 'in-til']/text()\")\n",
    "        occp_loc = webs.xpath(\"//div/div[1]/div[@class = 'in-msg']/div[1]/span[1]/text()\")\n",
    "        occp_edu = webs.xpath(\"//div/div[1]/div[@class = 'in-msg']/div[1]/span[2]/text()\")\n",
    "        occp_scal = webs.xpath(\"//div/div[1]/div[@class = 'in-msg']/div[1]/span[3]/text()\")\n",
    "        occp_detail = webs.xpath(\"//div/div[1]/div[3]\")\n",
    "        occp_details = []\n",
    "        for j in occp_detail:\n",
    "            occp_details.append(etree.tostring(j, encoding = 'Unicode'))\n",
    "        occpdata = occpdata.append([{'company':occp_comp,\n",
    "                                     'occupation':occp_name,\n",
    "                                     'location':occp_loc,\n",
    "                                     'edu_ask':occp_edu,\n",
    "                                     'scal':occp_scal,\n",
    "                                     'detail':occp_details}])\n",
    "    else:\n",
    "        error_web.append(i)\n",
    "        print(\"error: %s\\n\" %i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<div class=\"nav-fs\">\n",
      "</div>\n",
      "\n",
      "<div class=\"in-detail cutom_font&#xA0;\"><p>岗位职责：</p>\n",
      "\n",
      "<p>1.协助法律主管做好公司日常法律事务；</p>\n",
      "\n",
      "<p>2.协助法律主管参与收集整理各类合同文本，草拟公司标准格式合同文本和非标准格式合同文本以及其它法律文件；</p>\n",
      "\n",
      "<p>3.协助法律主管办理公司工商、专利和商标等事务工作；</p>\n",
      "\n",
      "<p>4.协助法律主管办理公司业务经营所涉的抵押、质押、过户等法律手续；</p>\n",
      "\n",
      "<p>5.参与并协助法律主管采集并保存潜在法律纠纷的证据资料；</p>\n",
      "\n",
      "<p>6.负责分析与本公司业务相关的法律信息及文件，并结合公司情况提出法律意见；</p>\n",
      "\n",
      "<p>7.参与并协助法律主管负责公司法律培训和普法工作；</p>\n",
      "\n",
      "<p>8.参与办理诉讼案件的内外部法律手续以及执行程序法律后续工作；</p>\n",
      "\n",
      "<p>9.负责公司案件卷宗的归档整理工作；</p>\n",
      "\n",
      "<p>10.按时完成领导交办的其他任务。</p>\n",
      "\n",
      "<p>任职资格要求：</p>\n",
      "\n",
      "<p>1.具有法学类专业本科及以上学历；</p>\n",
      "\n",
      "<p>2.熟悉公司法、合同法、担保法、劳动合同法等与进出口业务相关的法律、法规，能运用各种法律工具；</p>\n",
      "\n",
      "<p>3.有良好的沟通能力、工作认真仔细，有责任心，稳重，处事冷静；</p>\n",
      "\n",
      "<p>4.有一定的英语水平，文字组织能力强，掌握办公自动化；</p>\n",
      "\n",
      "<p>5.通过国家司法考试或通过全国企业法律顾问资格考试的优先考虑。</p></div>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "occp_detail = webs.xpath(\"//div/div[1]/div[3]\")\n",
    "for i in occp_detail:\n",
    "    print (etree.tostring(i, encoding = 'Unicode'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "occpdata.to_csv(\"newdats.csv\", index = False, encoding = 'utf-8_sig')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
